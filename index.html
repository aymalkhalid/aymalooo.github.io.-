<!DOCTYPE html>
<html>
<head>
    <title>Home</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <script src="https://cdn.jsdelivr.net/npm/elevenlabs-js@latest/dist/elevenlabs.min.js"></script>
    <!-- favicon -->
    <link rel="icon" href="https://demo.mobilize-ai.com/img/logo/ev3_logo.svg" />
</head>
<body>
   <!-- Add the logo container at the top -->
   <div class="logo-container">
    <img src="https://demo.mobilize-ai.com/img/logo/dish_logo.svg" alt="DISH Logo" class="logo">
    <img src="https://demo.mobilize-ai.com/img/logo/ev3_logo.svg" alt="Mobilize AI Logo" class="logo">
</div>  
   <!-- Add this right after the opening <body> tag -->
  <div id="modal-overlay" class="modal-overlay">
    <div class="modal-content">
      <h2>Welcome to DISH Agent</h2>
      <p>Please enter your information to continue</p>
      
      <form id="user-info-form">
        <div class="form-group">
          <label for="firstName">Applicant Name</label>
          <input type="text" id="firstName" name="firstName" required>
        </div>
        
        <div class="form-group">
          <label for="position">Applicant Position</label>
          <input type="text" id="position" name="position" required>
        </div>
        
        <div class="form-group">
          <label for="lastJob">Last Job Company</label>
          <input type="text" id="lastJob" name="lastJob" required>
        </div>
        <!--  -->
        <div class="form-group">
          <label for="elevenLabsKey">ElevenLabs API Key</label>
          <input type="password" id="elevenLabsKey" required>
          <small>Get your key at <a href="https://elevenlabs.io/app" target="_blank">elevenlabs.io</a></small>
        </div>

        <button type="submit" class="submit-btn">Start Conversation</button>
      </form>
    </div>
  </div>
<!-- Add this right after the df-messenger element -->
<div id="speech-status" style="position: fixed; bottom: 115px; left: 50%; transform: translateX(-50%); color: #007bff; font-weight: bold; background-color: rgba(255,255,255,0.9); padding: 10px 15px; border-radius: 8px; display: none; box-shadow: 0 4px 10px rgba(0,0,0,0.15); font-size: 16px; z-index: 1002;"></div>
   <!--End of Popup  -->
    <link rel="stylesheet" href="https://www.gstatic.com/dialogflow-console/fast/df-messenger/prod/v1/themes/df-messenger-default.css">
    <script src="https://www.gstatic.com/dialogflow-console/fast/df-messenger/prod/v1/df-messenger.js"></script>
    <df-messenger
      project-id="agent-demos-v1"
      agent-id="fa47c7e0-4ea7-431d-ba0f-a62126f4070a"
      language-code="en"
      max-query-length="-1"
      allow-feedback="all"
      intent="WELCOME"
      >
      <df-messenger-chat
       chat-title="DISH Ev3 Global"
       >
      </df-messenger-chat>
   </df-messenger>
    <div id="error-message" style="position: fixed; bottom: 85px; right: 20px; color: red;"></div>
    <style>
      df-messenger {
        z-index: 999;
        position: fixed;
        --df-messenger-font-color: #000;
        --df-messenger-font-family: Google Sans;
        --df-messenger-chat-background: #f3f6fc;
        --df-messenger-message-user-background: #d3e3fd;
        --df-messenger-message-bot-background: #fff;
        top: 10%;
        right: 60%;
        bottom: 200%;
        left: 35%;
        width: 30%;
        height: 45%;
      }
        /* Logo container styling */
        .logo-container {
            position: fixed;
            top: 20px;
            left: 0;
            right: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 30px;
            z-index: 1005;
        }
        
        .logo {
            height: 50px;
            width: auto;
            object-fit: contain;
            transition: all 0.3s ease;
        }
        
        .logo:hover {
            transform: scale(1.05);
        }      
      .voice-button {
        position: fixed;
        bottom: 20px;
        right: 20px;
        background-color: #007bff;
        color: white;
        border: none;
        border-radius: 50%;
        width: 60px;
        height: 60px;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 24px;
        cursor: pointer;
        transition: background-color 0.3s;
      }
      .voice-button:focus {
        outline: none;
      }
      .voice-button.listening {
        background-color: #ff0000;
        animation: pulse-animation 1s infinite;
      }
      .pulse {
        animation: pulse-animation 2s infinite;
      }
      @keyframes pulse-animation {
        0% {
          transform: scale(1);
        }
        50% {
          transform: scale(1.2);
        }
        100% {
          transform: scale(1);
        }
      }
      .feedback-wrapper {
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
        gap: 10px;
      }
      .feedback-wrapper label {
        display: flex;
        align-items: center;
      }
      .feedback-wrapper input[type="checkbox"] {
        margin-right: 10px;
      }
      .feedback-wrapper button {
        background-color: #007bff;
        color: white;
        border: none;
        padding: 10px 20px;
        border-radius: 5px;
        cursor: pointer;
        font-size: 16px;
        margin-top: 10px;
      }
      .feedback-wrapper button:hover {
        background-color: #0056b3;
      }
      .feedback-wrapper button:focus {
        outline: none;
      }
      .feedback-wrapper textarea {
        width: 100%;
        height: 100px;
        padding: 10px;
        border: 1px solid #ccc;
        border-radius: 5px;
        font-size: 16px;
        resize: vertical;
      }
      .feedback-wrapper textarea:focus {
        border-color: #007bff;
        outline: none;
      }
      #speech-status {
        transition: opacity 0.3s ease;
        opacity: 0.9;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
      }
      @keyframes blink {
        0%, 100% { opacity: 1; }
        50% { opacity: 0.5; }
      }
      #speech-status.listening {
        animation: blink 1.5s infinite;
      }
      .modal-overlay {
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background-color: rgba(0, 0, 0, 0.7);
        display: flex;
        justify-content: center;
        align-items: center;
        z-index: 1000;
      }
      .modal-content {
        background-color: white;
        padding: 30px;
        border-radius: 10px;
        max-width: 500px;
        width: 90%;
        box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);
      }
      .modal-content h2 {
        margin-top: 0;
        color: #007bff;
      }
      .form-group {
        margin-bottom: 20px;
      }
      .form-group label {
        display: block;
        margin-bottom: 5px;
        font-weight: bold;
      }
      .form-group input {
        width: 100%;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 5px;
        font-size: 16px;
      }
      .submit-btn {
        background-color: #007bff;
        color: white;
        border: none;
        padding: 12px 20px;
        border-radius: 5px;
        font-size: 16px;
        cursor: pointer;
        width: 100%;
      }
      .submit-btn:hover {
        background-color: #0056b3;
      }
      .voice-button.ready-to-send {
        box-shadow: 0 0 15px rgba(255, 0, 0, 0.7);
        transform: scale(1.1);
      }
      .mic-controls {
    position: fixed;
    z-index: 1001;
    display: flex;
    flex-direction: column;
    align-items: center;
    bottom: 20px;
    left: 50%;
    transform: translateX(-50%);
    transition: all 0.3s ease;
  }
  
  .initialize-mic-button {
    padding: 12px 24px;
    background-color: #007bff;
    color: white;
    border: none;
    border-radius: 30px;
    cursor: pointer;
    font-weight: 600;
    font-size: 16px;
    transition: all 0.3s ease;
    box-shadow: 0 4px 12px rgba(0,123,255,0.3);
    display: flex;
    align-items: center;
    gap: 10px;
  }
  
  .initialize-mic-button:hover {
    background-color: #0069d9;
    transform: translateY(-2px);
    box-shadow: 0 6px 16px rgba(0,123,255,0.4);
  }
  
  /* Microphone button after initialization */
  #mic-active-controls {
    display: flex;
    flex-direction: column;
    align-items: center;
    width: 100%;
  }
  
  .mic-button {
    border: none;
    background: none;
    cursor: pointer;
    padding: 0;
    display: flex;
    flex-direction: column;
    align-items: center;
    gap: 8px;
    transition: all 0.3s ease;
    width: auto;
  }
  
  .mic-button:focus {
    outline: none;
  }
  
  .mic-button-inner {
    position: relative;
    width: 64px;
    height: 64px;
    border-radius: 50%;
    display: flex;
    align-items: center;
    justify-content: center;
    background-color: white;
    box-shadow: 0 4px 15px rgba(0,0,0,0.15);
    transition: all 0.3s ease;
  }
  
  .mic-icon {
    font-size: 24px;
    color: #5f6368;
    transition: all 0.3s ease;
  }
  
  /* Inactive state */
  .mic-inactive .mic-button-inner {
    background-color: white;
  }
  .mic-inactive .mic-icon {
    color: #5f6368;
  }
  #mic-status-text {
    font-size: 14px;
    font-weight: 500;
    color: #5f6368;
    transition: all 0.3s ease;
  }
  
  /* Active state */
  .mic-active .mic-button-inner {
    background-color: #ea4335;
    animation: pulse-mic 2s infinite;
  }
  .mic-active .mic-icon {
    color: white;
  }
  .mic-active #mic-status-text {
    color: #ea4335;
    font-weight: 600;
  }
  
  /* Pulse animation for active mic */
  @keyframes pulse-mic {
    0% {
      box-shadow: 0 0 0 0 rgba(234, 67, 53, 0.4);
    }
    70% {
      box-shadow: 0 0 0 10px rgba(234, 67, 53, 0);
    }
    100% {
      box-shadow: 0 0 0 0 rgba(234, 67, 53, 0);
    }
  }
  
  /* Sound wave animation */
/* Sound wave container - positioned outside the button */
.sound-wave {
  position: absolute;
  width: 200%; /* Wider than the button to extend outside */
  height: 200%; /* Taller than the button to extend outside */
  display: flex;
  align-items: center;
  justify-content: center;
  opacity: 0;
  transition: opacity 0.3s ease;
  top: -50%; /* Move up to center vertically */
  left: -50%; /* Move left to center horizontally */
  pointer-events: none; /* Allow clicks to pass through */
}

.mic-active .sound-wave {
  opacity: 1;
}
  
  .sound-wave span {
  position: absolute;
  width: 4px; /* Slightly thicker for better visibility */
  background-color: rgba(234, 67, 53, 0.8); /* Red color to match active mic */
  border-radius: 6px;
  animation: sound-wave-animation 1.2s infinite ease-in-out;
  box-shadow: 0 0 4px rgba(234, 67, 53, 0.5); /* Subtle glow effect */
}
  
/* Position wave bars outside the button */
.sound-wave span:nth-child(1) {
  height: 15px;
  right: 20px; /* Position to the right of the button */
  top: 40%; /* Position vertically */
  animation-delay: 0s;
}

.sound-wave span:nth-child(2) {
  height: 25px;
  right: 30px; /* Position to the right of the button */
  top: 35%; /* Position vertically */
  animation-delay: 0.2s;
}

.sound-wave span:nth-child(3) {
  height: 20px;
  right: 40px; /* Position to the right of the button */
  top: 38%; /* Position vertically */
  animation-delay: 0.4s;
}

.sound-wave span:nth-child(4) {
  height: 15px;
  right: 50px; /* Position to the right of the button */
  top: 42%; /* Position vertically */
  animation-delay: 0.6s;
}

/* Left side sound waves (optional addition) */
.sound-wave span:nth-child(5) {
  height: 18px;
  left: 20px; /* Position to the left of the button */
  top: 40%; /* Position vertically */
  animation-delay: 0.3s;
}

.sound-wave span:nth-child(6) {
  height: 23px;
  left: 30px; /* Position to the left of the button */
  top: 35%; /* Position vertically */
  animation-delay: 0.1s;
}

.sound-wave span:nth-child(7) {
  height: 16px;
  left: 40px; /* Position to the left of the button */
  top: 38%; /* Position vertically */
  animation-delay: 0.5s;
}

.sound-wave span:nth-child(8) {
  height: 12px;
  left: 50px; /* Position to the left of the button */
  top: 42%; /* Position vertically */
  animation-delay: 0.7s;
}
  @keyframes sound-wave-animation {
  0% {
    transform: scaleY(0.4);
    opacity: 0.7;
  }
  50% {
    transform: scaleY(1);
    opacity: 1;
  }
  100% {
    transform: scaleY(0.4);
    opacity: 0.7;
  }
}
/* Warning state for mic button */
.mic-warning .mic-button-inner {
  box-shadow: 0 0 15px rgba(234, 67, 53, 0.7);
}

/* Add warning pulsing animation */
@keyframes warning-pulse {
  0% {
    box-shadow: 0 0 0 0 rgba(234, 67, 53, 0.7);
  }
  70% {
    box-shadow: 0 0 0 10px rgba(234, 67, 53, 0);
  }
  100% {
    box-shadow: 0 0 0 0 rgba(234, 67, 53, 0);
  }
}

.mic-warning .mic-button-inner {
  animation: warning-pulse 1s ease-in-out;
}
</style>
<!--  -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Apply blur to background elements
      const dfMessenger = document.querySelector('df-messenger');
      const micControls = document.getElementById('mic-controls');
      
      if (dfMessenger) {
        dfMessenger.classList.add('blur-background');
      }
      
      // Hide the mic controls initially
      if (micControls) {
        micControls.style.display = 'none';
      }
      
      // Handle form submission
      const userInfoForm = document.getElementById('user-info-form');
      if (userInfoForm) {
        userInfoForm.addEventListener('submit', function(event) {
          event.preventDefault();
          
          // Get form values
          const firstName = document.getElementById('firstName').value.trim();
          const position = document.getElementById('position').value.trim();
          const lastJob = document.getElementById('lastJob').value.trim();
          
          // Validate inputs
          if (!firstName || !position || !lastJob) {
            alert('Please fill in all fields');
            return;
          }
          
          // Store values in global variables to be used by setUserInformation
          window.userInfo = {
            firstName: firstName,
            position: position,
            lastJob: lastJob
          };
          
          // Remove blur effect
          if (dfMessenger) {
            dfMessenger.classList.remove('blur-background');
          }
          
          // Hide modal
          document.getElementById('modal-overlay').style.display = 'none';
          
          // Show mic controls and position them
          if (micControls) {
                micControls.style.display = 'flex';
                // Position the mic controls properly to avoid overlap
                const dfRect = dfMessenger.getBoundingClientRect();
                micControls.style.position = 'fixed';
                
                // Calculate the position - ensure it's below the Dialogflow widget with adequate spacing
                // but also check if it would be too low on the screen
                const proposedTop = dfRect.bottom + 30;
                const screenHeight = window.innerHeight;
                
                if (proposedTop > screenHeight - 150) {
                  // If it would be too low, position it at the bottom of the screen with padding
                  micControls.style.bottom = '30px';
                  micControls.style.top = 'auto';
                } else {
                  // Otherwise position it below the widget
                  micControls.style.top = proposedTop + 'px';
                  micControls.style.bottom = 'auto';
                }
                
                micControls.style.left = '50%';
              }
          
          // Initialize the chatbot with user info
          initializeAgent();
        });
      }
    });
    
    function initializeAgent() {
      setUserInformation();
    }
  </script>
<!-- //  -->

<script>
  // Speech recognition setup
  const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
  
  // Configure recognition settings for better quality
  recognition.lang = 'en-US';
  recognition.interimResults = true; 
  recognition.maxAlternatives = 5;  
  recognition.continuous = true;   
  
  // Global variables
  let isRecognitionActive = false;
  let currentTranscript = '';
  let allTranscripts = [];
  let speechWasInterrupted = false;
  
  /**
   * Shows or hides the listening status indicator
   */
   function showListeningStatus(isListening) {
    const speechStatus = document.getElementById('speech-status');
    const micToggleButton = document.getElementById('mic-toggle-button');
    
    if (speechStatus) {
      if (isListening) {
        // speechStatus.textContent = "Listening...";
        // speechStatus.style.display = "block";
        // speechStatus.classList.add('listening');
      } else {
        speechStatus.style.display = "none";
      }
    }
    
    // Also update the mic button visual state
    if (micToggleButton && isListening) {
      micToggleButton.classList.add('mic-active');
      micToggleButton.classList.remove('mic-inactive');
    } else if (micToggleButton) {
      micToggleButton.classList.add('mic-inactive');
      micToggleButton.classList.remove('mic-active');
    }
  }

  function showSpeechStatus(isSpeaking) {
    const speechStatus = document.getElementById('speech-status');
    
    if (speechStatus) {
      if (isSpeaking) {
        speechStatus.textContent = "Speaking...";
        speechStatus.style.display = "block";
        speechStatus.classList.add('speaking');
      } else {
        speechStatus.style.display = "none";
      }
    }
  }
  /**
   * Handle the end of a recognition session
   */
// Update the result event handler to track speech detection
recognition.addEventListener('result', (event) => {
  // Mark that speech was detected in this session
  speechDetectedInSession = true;
  
  console.log("Speech recognition result:", event.results);
  
  const latestResult = event.results[event.results.length - 1];
  const isFinal = latestResult.isFinal;
  
  // Find the alternative with highest confidence
  let bestMatch = latestResult[0].transcript;
  let highestConfidence = latestResult[0].confidence;
  
  // Check all alternatives for better accuracy
  for (let i = 1; i < latestResult.length; i++) {
    if (latestResult[i].confidence > highestConfidence) {
      bestMatch = latestResult[i].transcript;
      highestConfidence = latestResult[i].confidence;
    }
  }
  
  // Process final results only (when speech segment is complete)
  if (isFinal) {
    currentTranscript = bestMatch;
    allTranscripts.push(bestMatch);
    console.log("Final transcript segment:", bestMatch, "Confidence:", highestConfidence);
  } else {
    // For interim results, just log but don't add to transcript collection
    console.log("Interim transcript:", bestMatch, "Confidence:", highestConfidence);
  }
});

// Update the recognition.end event handler - only show warning when mic has stopped
recognition.addEventListener('end', () => {
  // Always clear the speech detection timer when recognition ends
  clearTimeout(speechDetectionTimer);
  
  if (!isRecognitionActive) {
    // Recognition ended because user stopped it
    console.log("Transcript segments before sending:", JSON.stringify(allTranscripts));
    let fullTranscript = allTranscripts.length ? allTranscripts.join(' ').trim() : '';
    console.log("Combined transcript to send:", fullTranscript);
    
    if (fullTranscript) {
      // Send the transcript to Dialogflow
      const dfMessenger = document.querySelector('df-messenger');
      if (dfMessenger) {
        try {
          // Add the user's message to the chat UI
          dfMessenger.renderCustomText(fullTranscript, false);
          
          // Send the message to Dialogflow
          setTimeout(() => {
            dfMessenger.sendRequest('query', fullTranscript);
            console.log("✅ Transcript successfully sent to Dialogflow");
          }, 100);
        } catch (error) {
          console.error("Error sending transcript:", error);
        }
      }
    } 
    else if (!speechDetectedInSession) {
      console.log("⚠️ No speech detected in session");
      // Only show warning if no speech was detected in the entire session
      showNoSpeechDetectedWarning();
    }
    
    // Reset transcripts and speech detection flag
    currentTranscript = '';
    allTranscripts = [];
    speechDetectedInSession = false;
    
    // Update UI using the new UI elements and structure
    const micToggleButton = document.getElementById('mic-toggle-button');
    const micStatusText = document.getElementById('mic-status-text');
    
    if (micToggleButton) {
      micToggleButton.classList.remove('mic-active');
      micToggleButton.classList.add('mic-inactive');
    }
    
    if (micStatusText) {
      micStatusText.textContent = 'Start Speaking';
    }
    
    // Update recording status display
    showListeningStatus(false);
  } 
      else {
        // Recognition ended unexpectedly, restart it
        setTimeout(() => {
          try {
            recognition.start();
            console.log('Speech recognition restarted');
          } catch (error) {
            console.error('Error restarting speech recognition:', error);
            isRecognitionActive = false;
            showListeningStatus(false);
            
            // Update UI using the new UI elements and structure
            const micToggleButton = document.getElementById('mic-toggle-button');
            const micStatusText = document.getElementById('mic-status-text');
            
            if (micToggleButton) {
              micToggleButton.classList.remove('mic-active');
              micToggleButton.classList.add('mic-inactive');
            }
            
            if (micStatusText) {
              micStatusText.textContent = 'Start Speaking';
            }
          }
        }, 10);
      }
    });

  /**
   * Process speech recognition results with improved accuracy
   */
// Update the result event handler to track speech detection
recognition.addEventListener('result', (event) => {
  // Mark that speech was detected in this session
  speechDetectedInSession = true;
  
  console.log("Speech recognition result:", event.results);
  
  const latestResult = event.results[event.results.length - 1];
  const isFinal = latestResult.isFinal;
  
  // Find the alternative with highest confidence
  let bestMatch = latestResult[0].transcript;
  let highestConfidence = latestResult[0].confidence;
  
  // Check all alternatives for better accuracy
  for (let i = 1; i < latestResult.length; i++) {
    if (latestResult[i].confidence > highestConfidence) {
      bestMatch = latestResult[i].transcript;
      highestConfidence = latestResult[i].confidence;
    }
  }
  
  // Process final results only (when speech segment is complete)
  if (isFinal) {
    currentTranscript = bestMatch;
    allTranscripts.push(bestMatch);
    console.log("Final transcript segment:", bestMatch, "Confidence:", highestConfidence);
  } else {
    // For interim results, just log but don't add to transcript collection
    console.log("Interim transcript:", bestMatch, "Confidence:", highestConfidence);
  }
});

  /**
   * Handle recognition errors with better error messages
   */
  recognition.addEventListener('error', (event) => {
    console.error('Speech recognition error detected:', event.error);
    showError('Speech recognition error: ' + event.error);
    
    // Update UI
    isRecognitionActive = false;
    showListeningStatus(false);
    
    const micToggle = document.getElementById('mic-toggle');
    if (micToggle) {
      micToggle.checked = false;
    }
    const micStatusText = document.getElementById('mic-status-text');
    if (micStatusText) {
      micStatusText.textContent = 'Unmute Mic';
    }
  });
  
  /**
   * Stop recognition completely
   */
  function stopRecognition() {
    isRecognitionActive = false;
    
    try {
      recognition.stop();
    } catch (e) {
      console.error("Error stopping recognition:", e);
    }
    
    showListeningStatus(false);
    
    currentTranscript = '';
    allTranscripts = [];
  }
  
  /**
   * Show error message briefly
   */
  function showError(message) {
    const errorMessage = document.getElementById('error-message');
    if (errorMessage) {
      errorMessage.textContent = message;
      errorMessage.style.display = 'block';
      
      // Hide after 3 seconds
      setTimeout(() => {
        errorMessage.style.display = 'none';
      }, 3000);
    }
  }
</script>
<!--  -->

<!--  -->
<script>
  /**
   * @fileoverview DISH Agent Dialogflow Integration Script
   * 
   * This script manages the integration between the web interface and Dialogflow CX,
   * handling speech synthesis, voice feedback, user information management, and 
   * custom feedback collection for the DISH Networks agent interview simulation.
   * 
   * @version 1.0.0
   * @author DISH Networks Team
   * 
   * ===== GLOBAL VARIABLES =====
   * 
   * @var {Array} speechSynthesisVoices - Stores available speech synthesis voices
   * @var {boolean} voicesLoaded - Flag indicating if voices have been loaded
   * @var {boolean} isFirstResponse - Flag for special handling of first AI response
   * @var {number} FlagWelcome - Tracks welcome message state (0: not sent, 1: preparing, 2: sent)
   * @var {string} AIMessage - Stores the most recent AI response text
   * @var {Array} feedbackTextsList - Collects all feedback texts submitted by users
   * @var {Array} selectedOptionsList - Collects all feedback options selected by users
   * 
   * ===== MAIN COMPONENTS =====
   * 
   * 1. CUSTOM FEEDBACK ELEMENT
   *    - Creates a custom web component for collecting user feedback
   *    - Renders checkboxes for quick feedback options and text area for detailed comments
   *    - Dispatches custom events when feedback is submitted
   * 
   * 2. SPEECH SYNTHESIS MANAGEMENT
   *    - Loads and manages available speech synthesis voices
   *    - Handles text-to-speech conversion for AI responses
   *    - Manages speaking status indicators
   * 
   * 3. DIALOGFLOW INTEGRATION
   *    - Listens for responses from Dialogflow Messenger
   *    - Processes and speaks AI responses
   *    - Handles user information for personalized interactions
   * 
   * 4. PARAMETER MANAGEMENT
   *    - Sets query parameters for Dialogflow with user information
   *    - Passes feedback data to Dialogflow for analytics
   * 
   * ===== DETAILED WORKFLOW =====
   * 
   * 1. On page load:
   *    - Attempts to load speech synthesis voices
   *    - Sets up event listeners for voice changes
   * 
   * 2. When user submits the welcome form:
   *    - User information is stored and passed to Dialogflow
   *    - Welcome message is triggered
   * 
   * 3. When Dialogflow responds:
   *    - Response text is captured and spoken aloud
   *    - First response gets special handling (delayed speaking if needed)
   * 
   * 4. When user provides feedback:
   *    - Feedback text and selected options are stored
   *    - Data is passed to Dialogflow via query parameters
   * 
   * ===== FUNCTION REFERENCE =====
   */
  

  let speechSynthesisVoices = [];
  let voicesLoaded = false;
  let isFirstResponse = true;
  let FlagWelcome = 0;
  let AIMessage = '';
  // Global arrays to store feedback text and selected options
  const feedbackTextsList = [];
  const selectedOptionsList = [];
  const feedbackAITextList = [];
  /**
   * @class CustomFeedbackElement
   * @extends HTMLElement
   * @description Custom web component that creates a feedback interface with checkboxes and text input
   */    
  class CustomFeedbackElement extends HTMLElement {
    constructor() {
      super();
      this.renderRoot = this.attachShadow({ mode: 'open' });
    }
/**
 * @method connectedCallback
 * @description Lifecycle method that runs when the element is added to the DOM
 * Renders the feedback interface with checkboxes and text area, and adds a submit button.
 * Dispatches a custom event with feedback data when the submit button is clicked. 
 */
    connectedCallback() {
      const wrapper = document.createElement('div');
      wrapper.classList.add('feedback-wrapper');

      const options = [
        "Inaccurate", "Partially Accurate", "Unhelpful", "Context Lost", 
        "Unnatural Language", "Repetitive", "Edge Case Failure", "Human Escalation Needed"
      ];

      options.forEach(option => {
        const label = document.createElement('label');
        const checkbox = document.createElement('input');
        checkbox.type = 'checkbox';
        checkbox.value = option;
        label.appendChild(checkbox);
        label.appendChild(document.createTextNode(option));
        wrapper.appendChild(label);
      });

      wrapper.appendChild(document.createElement('br'));
      wrapper.appendChild(document.createElement('br'));
      const textArea = document.createElement('textarea');
      
      textArea.placeholder = 'Enter your feedback here...';
      textArea.style.width = '100%';
      textArea.style.height = '100px';
      textArea.id = 'feedback-text';
      wrapper.appendChild(textArea);

      const button = document.createElement('button');
      button.innerText = 'Submit';
      button.addEventListener('click', () => {
        this._onSubmitClick();
      });
      wrapper.appendChild(button);

      this.renderRoot.appendChild(wrapper);
    }

    _onSubmitClick() {
      const feedbackText = this.renderRoot.getElementById('feedback-text').value;
      const checkboxes = this.renderRoot.querySelectorAll('input[type="checkbox"]');
      const selectedOptions = Array.from(checkboxes)
        .filter(checkbox => checkbox.checked)
        .map(checkbox => checkbox.value);
      
      feedbackTextsList.push(feedbackText);
      selectedOptionsList.push(selectedOptions);
      feedbackAITextList.push(AIMessage)
      // Dispatch custom event with feedback data
      const event = new CustomEvent("df-custom-submit-feedback-clicked", {
        detail: JSON.stringify({
          "feedback": feedbackTextsList,
          "selectedOptions": selectedOptionsList,
          "feedbackAITextList": feedbackAITextList
        }),
        bubbles: true,
        composed: true,
      });
      this.dispatchEvent(event);
    }
  }
/**
 * @function funsetQueryParameters
 * @description Sets query parameters for Dialogflow with feedback data
 * @param {Array} feedbackTextsList - List of feedback text submitted by users
 * 
 */ 
  (function() {customElements.define('df-external-custom-feedback', CustomFeedbackElement);})();

  document.addEventListener('df-custom-submit-feedback-clicked', (event) => {
    console.log('Feedback submitted:', event.detail); 
    const submitted_feedback = JSON.parse(event.detail);
    funsetQueryParameters(submitted_feedback.feedback, submitted_feedback.selectedOptions, submitted_feedback.feedbackAITextList);
  });   
/**
 * @function loadVoices
 * @description Loads available speech synthesis voices and sets the global voicesLoaded flag
 * @returns {boolean} - Flag indicating if voices have been loaded
 * 
 */ 
  function loadVoices() {
    console.log('Loading voices...');
    speechSynthesisVoices = window.speechSynthesis.getVoices();
    voicesLoaded = speechSynthesisVoices.length > 0;
    console.log(`${speechSynthesisVoices.length} voices loaded, voicesLoaded=${voicesLoaded}`);
    return voicesLoaded;
  }
  // 
  loadVoices();
/**
 * @function showSpeechStatus
 * @description Displays the speaking status indicator
 * @param {boolean} isSpeaking - Flag indicating if speech synthesis is active
 */
  if (window.speechSynthesis.onvoiceschanged !== undefined) {
    window.speechSynthesis.onvoiceschanged = function() {
      voicesLoaded = loadVoices();
      console.log('Voices changed event triggered');
    };
  }
/**
 * @function showSpeechStatus
 * @description Displays the speaking status indicator
 * @param {boolean} isSpeaking - Flag indicating if speech synthesis is active
 */
 function showSpeechStatus(isSpeaking) {
  const speechStatus = document.getElementById('speech-status');
  if (speechStatus) {
    speechStatus.style.display = "none";
  }
}
// Add these global variables at the top of your script section with other globals
// Remove these lines
let audioElement = null;
const ELEVEN_LABS_API_KEY = document.getElementById('elevenLabsKey').value; // API Key for ElevenLabs
console.log('ElevenLabs API Key:', ELEVEN_LABS_API_KEY);
const VOICE_ID = 'tnSpp4vdxKPjI9w0GnoV'; // HOPE Voice ID for ElevenLabs API

// Get the current API key from the input field
function getElevenLabsApiKey() {
  console.log('Getting ElevenLabs API Key:', document.getElementById('elevenLabsKey').value);
  return document.getElementById('elevenLabsKey').value;
}

/**
 * @function speakText
 * @description Converts text to speech using ElevenLabs API
 * @param {string} text - Text to be spoken
 */
async function speakText(text) {
  if (!text || typeof text !== 'string') {
    console.log('Invalid text to speak:', text);
    return;
  }
  
  console.log('Speaking with ElevenLabs API (direct):', text);
  
  // Show speaking status
  showSpeechStatus(true);
  
  try {
    // Get the API key at the time of the request
    const apiKey = getElevenLabsApiKey();
    if (!apiKey) {
      throw new Error('No ElevenLabs API key provided');
    }
    
    // Stop any current playback
    if (audioElement) {
      audioElement.pause();
      audioElement = null;
    }
    
    // Call ElevenLabs streaming API
    const response = await fetch(`https://api.elevenlabs.io/v1/text-to-speech/${VOICE_ID}/stream`, {
      method: 'POST',
      headers: {
        'Accept': 'audio/mpeg',
        'Content-Type': 'application/json',
        'xi-api-key': apiKey
      },
      body: JSON.stringify({
        text: text,
        model_id: 'eleven_monolingual_v1',
        voice_settings: {
          stability: 0.5,
          similarity_boost: 0.5
        }
      })
    });
    
    if (!response.ok) {
      throw new Error(`ElevenLabs API error: ${response.status}`);
    }
    
    // Get audio data
    const audioBlob = await response.blob();
    const audioUrl = URL.createObjectURL(audioBlob);
    
    // Create audio element for playback
    const audio = new Audio(audioUrl);
    audio.onended = () => {
      console.log('Finished playing audio');
      showSpeechStatus(false);
      URL.revokeObjectURL(audioUrl);
    };
    
    // Store reference to cancel if needed
    audioElement = audio;
    
    // Start playback
    audio.play();
    
  } catch (error) {
    console.error('ElevenLabs API error:', error);
    showSpeechStatus(false);
    
    // Fallback to browser's speech synthesis
    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = 'en-US';
    utterance.rate = 1.0;
    utterance.pitch = 1.2;
    window.speechSynthesis.speak(utterance);
  }
}
/**
 * Listen to Dialogflow response events and process AI messages, with special handling for the first response. 
 */  
  const dfMessenger = document.querySelector('df-messenger');
  dfMessenger.addEventListener('df-response-received', (event) => {
      console.log(`Response received (isFirstResponse=${isFirstResponse}):`, 
        event.detail.raw.queryResult.diagnosticInfo);
      
      const responseData = event.detail;
      if (responseData.messages && responseData.messages.length > 0) {
        console.log('Message:', responseData.messages[0].text);
        AIMessage = responseData.messages[0].text;
        
        if (isFirstResponse) {
          isFirstResponse = false;
          console.log('Processing first response with special handling');
          
          if (!voicesLoaded) {
            loadVoices();
            setTimeout(() => {
              speakText(AIMessage);
            }, 300);
          } else {
            speakText(AIMessage);
          }
        } else {
          speakText(AIMessage);
        }
      }
      
      if (FlagWelcome === 0 && window.userInfo) {
        FlagWelcome = 1;
        setUserInformation();
      }
  });
/**
 * @function setUserInformation
 * @description Sets user information parameters for Dialogflow
 * 
 */  
  function setUserInformation() {
    const dfMessenger = document.querySelector('df-messenger');
    console.log("Setting user parameters");
    
    const userInfo = window.userInfo || {
      firstName: "Jonathan",
      position: "Wireless Customer Service Agent",
      lastJob: "Disney"
    };
    
    const queryParameters = {
      parameters: {
        job_applicant_first_name: userInfo.firstName,
        job_applicant_position: userInfo.position,
        job_applicant_last_Job: userInfo.lastJob,
      }
    };
    
    dfMessenger.setQueryParameters(queryParameters);
    console.log("Parameters set successfully:", userInfo);
    console.log("Flag", FlagWelcome);
    if (FlagWelcome === 0) {
      console.log("Sending welcome message");
      dfMessenger.sendRequest('query', 'Hello');
      FlagWelcome = 2;
    }
  }
/**
 * @function funsetQueryParameters
 * @description Sets query parameters for Dialogflow with feedback data For analytics and Improvements.
 * @param {Array} feedbackTextsList - List of feedback text submitted by users
 * @param {Array} selectedOptionsList - List of feedback options selected by users
 */  
  function funsetQueryParameters(feedbackTextsList, selectedOptionsList) {
    const dfMessenger = document.querySelector('df-messenger');
    const queryParameters = {
      parameters: {
        feedbackTextsList: feedbackTextsList,
        selectedOptionsList: selectedOptionsList,
        feedbackAITextList: feedbackAITextList
      }
    };
    dfMessenger.setQueryParameters(queryParameters);  
    console.log("Set query parameters with feedback and selected options");
    console.log("Feedback Texts List: ", feedbackTextsList);
    console.log("Selected Options List: ", selectedOptionsList);
    console.log("AI Message: ", AIMessage);
  }
</script>
<!-- Replace the current mic-controls div with this enhanced version -->
<div id="mic-controls" class="mic-controls">
  <!-- Initial state - request permission -->
  <button id="initialize-mic" class="initialize-mic-button">
    <i class="fas fa-microphone mic-status-icon"></i>
    Enable Microphone
  </button>
  
  <!-- Microphone control after initialization -->
  <div id="mic-active-controls" style="display: none;">
    <button id="mic-toggle-button" class="mic-button mic-inactive">
      <div class="mic-button-inner">
        <i class="fas fa-microphone mic-icon"></i>
        <div id="sound-wave" class="sound-wave">
          <span></span>
          <span></span>
          <span></span>
          <span></span>
          <span></span>
          <span></span>
          <span></span>
          <span></span>
        </div>
      </div>
      <span id="mic-status-text">Start Speaking</span>
    </button>
  </div>
</div>
<!--  -->
<script>
      let speechDetectionTimer = null;
      const SPEECH_DETECTION_TIMEOUT = 5000; // 5 seconds timeout
      let speechDetectedInSession = false;
      // Start recognition function
      // Modify the startSpeechRecognition function
      function startSpeechRecognition() {
  if (!isRecognitionActive) {
    // Stop any ongoing audio playback
    if (window.audioElement) {
      window.audioElement.pause();
      window.audioElement = null;
      console.log('ElevenLabs audio playback interrupted');
    }
    
    // Cancel any ongoing speech synthesis
    if (window.speechSynthesis.speaking) {
      window.speechWasInterrupted = true;
      console.log('Speech interrupted by user');
      window.speechSynthesis.cancel();
    }
    
    // Reset variables for new recording
    currentTranscript = '';
    allTranscripts = [];
    speechDetectedInSession = false; // Reset the speech detection flag
    
    // Start recognition
    try {
      recognition.start();
      isRecognitionActive = true;
      showListeningStatus(true);
      console.log('Speech recognition started');
      
    } catch (error) {
      console.error('Error starting speech recognition:', error);
      showError('Failed to start speech recognition');
      resetMicUI();
    }
  }
}
      // Add this new function to show the warning
      function showNoSpeechDetectedWarning() {
        const micToggleButton = document.getElementById('mic-toggle-button');
        const micStatusText = document.getElementById('mic-status-text');
        const speechStatus = document.getElementById('speech-status');
        
        // Show warning message
        if (speechStatus) 
        {
          speechStatus.textContent = "No speech detected. Please try again.";
          speechStatus.style.display = "block";
          speechStatus.style.color = "#ea4335"; // Red color for warning
          
          // Hide after 3 seconds
          setTimeout(() => {
            if (!isRecognitionActive) {
              speechStatus.style.display = "none";
            }
          }, 3000);
        }
        
        // Update mic status text
        if (micStatusText) {
          micStatusText.textContent = "No speech detected";
          micStatusText.style.color = "#ea4335";
          
          // Reset after 2 seconds
          setTimeout(() => {
            if (micToggleButton && !micToggleButton.classList.contains('mic-active')) {
              micStatusText.textContent = "Start Speaking";
              micStatusText.style.color = "#5f6368";
            }
          }, 2000);
        }
        
        // Add a subtle visual indication
        if (micToggleButton) {
          micToggleButton.classList.add('mic-warning');
          
          setTimeout(() => {
            micToggleButton.classList.remove('mic-warning');
          }, 2000);
        }
      }
    document.addEventListener('DOMContentLoaded', function() {
      // Reference the UI elements
      const initializeMicButton = document.getElementById('initialize-mic');
      const micActiveControls = document.getElementById('mic-active-controls');
      const micToggleButton = document.getElementById('mic-toggle-button');
      const micStatusText = document.getElementById('mic-status-text');
      
      // Mic initialization button
      if (initializeMicButton) {
        initializeMicButton.addEventListener('click', function() {
          // Request microphone permissions
          navigator.mediaDevices.getUserMedia({ audio: true })
            .then(function(stream) {
              console.log('Microphone permission granted');
              
              // Permission granted, show the active mic controls
              initializeMicButton.style.display = 'none';
              if (micActiveControls) {
                micActiveControls.style.display = 'flex';
              }
              
              // Release the stream since we're just checking permissions
              stream.getTracks().forEach(track => track.stop());
            })
            .catch(function(err) {
              console.error('Error getting microphone permission:', err);
              showError('Microphone access denied. Please allow microphone permissions and try again.');
            });
        });
      }
      
      // Mic toggle button
      if (micToggleButton) {
        micToggleButton.addEventListener('click', function() {
          // Toggle between active/inactive states
          const isCurrentlyActive = micToggleButton.classList.contains('mic-active');
          
          if (!isCurrentlyActive) {
            // Activate microphone
            micToggleButton.classList.remove('mic-inactive');
            micToggleButton.classList.add('mic-active');
            micStatusText.textContent = 'Listening...';
            // Start recognition
            startSpeechRecognition();
          } else {
            // Deactivate microphone
            micToggleButton.classList.remove('mic-active');
            micToggleButton.classList.add('mic-inactive');
            micStatusText.textContent = 'Start Speaking';
            // Stop recognition
            stopSpeechRecognition();
          }
        });
      }  
      // Update the function to match the naming convention used in the micControls script:
      function stopSpeechRecognition() {
        if (isRecognitionActive) {
          recognition.stop();
          isRecognitionActive = false;
          showListeningStatus(false);
          clearTimeout(speechDetectionTimer);
          console.log('Stop requested by user');
        }
      }
      
      // Reset mic UI to inactive state
      function resetMicUI() {
        const micToggleButton = document.getElementById('mic-toggle-button');
        const micStatusText = document.getElementById('mic-status-text');
        
        if (micToggleButton) {
          micToggleButton.classList.remove('mic-active');
          micToggleButton.classList.add('mic-inactive');
        }
        
        if (micStatusText) {
          micStatusText.textContent = 'Start Speaking';
        }
      }
      
      // Update the recognition end event to handle UI updates
      recognition.addEventListener('end', function() {
        if (!isRecognitionActive) {
          resetMicUI();
          // Rest of your existing recognition end handler...
        }
      });
    });
  </script>
</body>
</html>
